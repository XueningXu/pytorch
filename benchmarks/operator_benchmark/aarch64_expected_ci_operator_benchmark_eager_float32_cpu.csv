[
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.919
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.452
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.716
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.598
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.344
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.428
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        220.425
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        219.609
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        219.472
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        258.279
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        257.281
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        257.937
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "start: 0, end: 1000, step: 2.5, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "arange_start0_end1000_step2.5_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.847
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "start: 0, end: 1000, step: 2.5, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "arange_start0_end1000_step2.5_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "start: -1024, end: 2048, step: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "arange_start-1024_end2048_step1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.34
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "start: -1024, end: 2048, step: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "arange_start-1024_end2048_step1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, size: (2, 2), stride: (1, 1), storage_offset: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "as_strided_M8_N8_size(2,2)_stride(1,1)_storage_offset0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.31
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, size: (2, 2), stride: (1, 1), storage_offset: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "as_strided_M8_N8_size(2,2)_stride(1,1)_storage_offset0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 256, size: (32, 32), stride: (1, 1), storage_offset: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "as_strided_M256_N256_size(32,32)_stride(1,1)_storage_offset0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.241
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 256, size: (32, 32), stride: (1, 1), storage_offset: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "as_strided_M256_N256_size(32,32)_stride(1,1)_storage_offset0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, size: (64, 64), stride: (2, 2), storage_offset: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "as_strided_M512_N512_size(64,64)_stride(2,2)_storage_offset1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.285
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, size: (64, 64), stride: (2, 2), storage_offset: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "as_strided_M512_N512_size(64,64)_stride(2,2)_storage_offset1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        347.858
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        96.938
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        355.033
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        353.429
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingTrue_cudnnFalse_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        343.283
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        342.73
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_M1_N256_K3136_cpu_trainingFalse_cudnnFalse_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        376.346
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        131.998
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        511.903
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        509.108
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: True, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingTrue_cudnnFalse_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        467.322
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        466.829
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 3136, C: 256, device: cpu, training: False, cudnn: False",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "batchnorm_N3136_C256_cpu_trainingFalse_cudnnFalse_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3.102
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.983
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.419
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.476
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.358
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.349
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3.178
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.971
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.409
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "copy__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.416
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "copy__M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "copy__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        50.992
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "copy__M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "copy__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        50.309
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "copy__M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.float32, dtype_two: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div__M1_N1_K1_cpu_dtype_onetorch.float32_dtype_twotorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3.032
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.float32, dtype_two: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div__M1_N1_K1_cpu_dtype_onetorch.float32_dtype_twotorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.float32, dtype_two: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div__M64_N64_K64_cpu_dtype_onetorch.float32_dtype_twotorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        60.721
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.float32, dtype_two: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div__M64_N64_K64_cpu_dtype_onetorch.float32_dtype_twotorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.float32, dtype_two: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div__M64_N64_K128_cpu_dtype_onetorch.float32_dtype_twotorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        61.301
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.float32, dtype_two: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div__M64_N64_K128_cpu_dtype_onetorch.float32_dtype_twotorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.943
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.124
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.168
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.13
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.945
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.991
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.618
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.92
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.828
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.999
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sub_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.553
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        168.379
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        171.76
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "div_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.894
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_M1_N1_K1_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.783
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_M64_N64_K64_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.202
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.int32, dtype_two: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_M64_N64_K128_cpu_dtype_onetorch.int32_dtype_twotorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bool",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.bool",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logical_and_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.bool",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        78.249
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bool",
      "extra_info": {
        "input_config": "in_one: [64, 1, 64], in_two: [1, 64, 1], device: cpu, dtype: torch.bool",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logical_and_in_one[64,1,64]_in_two[1,64,1]_cpu_dtypetorch.bool",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.bool, dtype_two: torch.bool",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logical_and_M1_N1_K1_cpu_dtype_onetorch.bool_dtype_twotorch.bool",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.53
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype_one: torch.bool, dtype_two: torch.bool",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logical_and_M1_N1_K1_cpu_dtype_onetorch.bool_dtype_twotorch.bool",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.bool, dtype_two: torch.bool",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logical_and_M64_N64_K64_cpu_dtype_onetorch.bool_dtype_twotorch.bool",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        53.824
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype_one: torch.bool, dtype_two: torch.bool",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logical_and_M64_N64_K64_cpu_dtype_onetorch.bool_dtype_twotorch.bool",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.bool, dtype_two: torch.bool",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logical_and_M64_N64_K128_cpu_dtype_onetorch.bool_dtype_twotorch.bool",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        53.798
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype_one: torch.bool, dtype_two: torch.bool",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logical_and_M64_N64_K128_cpu_dtype_onetorch.bool_dtype_twotorch.bool",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "B: 2, M: 1, N: 8, K: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bmm_B2_M1_N8_K2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3.998
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "B: 2, M: 1, N: 8, K: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bmm_B2_M1_N8_K2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "B: 2, M: 1, N: 8, K: 2, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bmm_B2_M1_N8_K2_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3.95
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "B: 2, M: 1, N: 8, K: 2, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bmm_B2_M1_N8_K2_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "B: 128, M: 64, N: 32, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bmm_B128_M64_N32_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        204.992
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "B: 128, M: 64, N: 32, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bmm_B128_M64_N32_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "B: 128, M: 64, N: 32, K: 64, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bmm_B128_M64_N32_K64_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        236.873
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "B: 128, M: 64, N: 32, K: 64, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bmm_B128_M64_N32_K64_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "B: 2, M: 1, N: 8, K: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "baddbmm_B2_M1_N8_K2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.431
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "B: 2, M: 1, N: 8, K: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "baddbmm_B2_M1_N8_K2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "B: 2, M: 1, N: 8, K: 2, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "baddbmm_B2_M1_N8_K2_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.333
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "B: 2, M: 1, N: 8, K: 2, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "baddbmm_B2_M1_N8_K2_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "B: 128, M: 64, N: 32, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "baddbmm_B128_M64_N32_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        272.019
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "B: 128, M: 64, N: 32, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "baddbmm_B128_M64_N32_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "B: 128, M: 64, N: 32, K: 64, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "baddbmm_B128_M64_N32_K64_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        304.624
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "B: 128, M: 64, N: 32, K: 64, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "baddbmm_B128_M64_N32_K64_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, dim: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cat_sizes(1,1,1)_N2_dim0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.598
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, dim: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cat_sizes(1,1,1)_N2_dim0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cat_sizes(512,512,2)_N2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        101.983
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cat_sizes(512,512,2)_N2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cat_sizes(128,1024,2)_N2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        99.236
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cat_sizes(128,1024,2)_N2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 2, channels_per_group: 16, height: 16, width: 16, groups: 2, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size2_channels_per_group16_height16_width16_groups2_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.732
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 2, channels_per_group: 16, height: 16, width: 16, groups: 2, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size2_channels_per_group16_height16_width16_groups2_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 2, channels_per_group: 16, height: 16, width: 16, groups: 2, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size2_channels_per_group16_height16_width16_groups2_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.525
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 2, channels_per_group: 16, height: 16, width: 16, groups: 2, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size2_channels_per_group16_height16_width16_groups2_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 2, channels_per_group: 32, height: 32, width: 32, groups: 2, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size2_channels_per_group32_height32_width32_groups2_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.795
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 2, channels_per_group: 32, height: 32, width: 32, groups: 2, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size2_channels_per_group32_height32_width32_groups2_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 2, channels_per_group: 32, height: 32, width: 32, groups: 2, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size2_channels_per_group32_height32_width32_groups2_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        51.677
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 2, channels_per_group: 32, height: 32, width: 32, groups: 2, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size2_channels_per_group32_height32_width32_groups2_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 4, channels_per_group: 32, height: 32, width: 32, groups: 4, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size4_channels_per_group32_height32_width32_groups4_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        78.172
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 4, channels_per_group: 32, height: 32, width: 32, groups: 4, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size4_channels_per_group32_height32_width32_groups4_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 4, channels_per_group: 32, height: 32, width: 32, groups: 4, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size4_channels_per_group32_height32_width32_groups4_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.541
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 4, channels_per_group: 32, height: 32, width: 32, groups: 4, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size4_channels_per_group32_height32_width32_groups4_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 4, channels_per_group: 64, height: 64, width: 64, groups: 4, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size4_channels_per_group64_height64_width64_groups4_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        312.254
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 4, channels_per_group: 64, height: 64, width: 64, groups: 4, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size4_channels_per_group64_height64_width64_groups4_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 4, channels_per_group: 64, height: 64, width: 64, groups: 4, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size4_channels_per_group64_height64_width64_groups4_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        177.476
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 4, channels_per_group: 64, height: 64, width: 64, groups: 4, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size4_channels_per_group64_height64_width64_groups4_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 8, channels_per_group: 64, height: 64, width: 64, groups: 8, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size8_channels_per_group64_height64_width64_groups8_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1204.837
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 8, channels_per_group: 64, height: 64, width: 64, groups: 8, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size8_channels_per_group64_height64_width64_groups8_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 8, channels_per_group: 64, height: 64, width: 64, groups: 8, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size8_channels_per_group64_height64_width64_groups8_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        736.648
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 8, channels_per_group: 64, height: 64, width: 64, groups: 8, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size8_channels_per_group64_height64_width64_groups8_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 16, channels_per_group: 64, height: 64, width: 64, groups: 16, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size16_channels_per_group64_height64_width64_groups16_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5558.776
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 16, channels_per_group: 64, height: 64, width: 64, groups: 16, channel_last: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size16_channels_per_group64_height64_width64_groups16_channel_lastTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 16, channels_per_group: 64, height: 64, width: 64, groups: 16, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size16_channels_per_group64_height64_width64_groups16_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        2999.558
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "batch_size: 16, channels_per_group: 64, height: 64, width: 64, groups: 16, channel_last: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "channel_shuffle_batch_size16_channels_per_group64_height64_width64_groups16_channel_lastFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, chunks: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "chunk_M8_N8_chunks2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.578
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, chunks: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "chunk_M8_N8_chunks2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, chunks: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "chunk_M256_N512_chunks2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.87
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, chunks: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "chunk_M256_N512_chunks2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, chunks: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "chunk_M512_N512_chunks2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.018
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, chunks: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "chunk_M512_N512_chunks2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 128, OC: 256, kernel: 3, stride: 1, N: 1, L: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv1d_IC128_OC256_kernel3_stride1_N1_L64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        162.87
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 128, OC: 256, kernel: 3, stride: 1, N: 1, L: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv1d_IC128_OC256_kernel3_stride1_N1_L64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 2, N: 4, L: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv1d_IC256_OC256_kernel3_stride2_N4_L64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        409.161
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 2, N: 4, L: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv1d_IC256_OC256_kernel3_stride2_N4_L64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        875.159
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "ConvTranspose2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1339.276
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "ConvTranspose2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv2dPointwise_IC256_OC256_stride1_N1_H16_W16_G1_pad0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        434.748
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv2dPointwise_IC256_OC256_stride1_N1_H16_W16_G1_pad0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 64, OC: 64, kernel: 3, stride: 1, N: 8, D: 4, H: 16, W: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv3d_IC64_OC64_kernel3_stride1_N8_D4_H16_W16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4938.652
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 64, OC: 64, kernel: 3, stride: 1, N: 8, D: 4, H: 16, W: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Conv3d_IC64_OC64_kernel3_stride1_N8_D4_H16_W16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 64, OC: 64, kernel: 3, stride: 1, N: 8, D: 4, H: 16, W: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "ConvTranspose3d_IC64_OC64_kernel3_stride1_N8_D4_H16_W16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10051.764
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 64, OC: 64, kernel: 3, stride: 1, N: 8, D: 4, H: 16, W: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "ConvTranspose3d_IC64_OC64_kernel3_stride1_N8_D4_H16_W16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dim: 1, M: 64, N: 64, diagonal: 0, out: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "diag_dim1_M64_N64_diagonal0_outTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.32
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dim: 1, M: 64, N: 64, diagonal: 0, out: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "diag_dim1_M64_N64_diagonal0_outTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dim: 2, M: 128, N: 128, diagonal: -10, out: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "diag_dim2_M128_N128_diagonal-10_outFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.532
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dim: 2, M: 128, N: 128, diagonal: -10, out: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "diag_dim2_M128_N128_diagonal-10_outFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dim: 1, M: 256, N: 256, diagonal: 20, out: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "diag_dim1_M256_N256_diagonal20_outTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        106.43
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dim: 1, M: 256, N: 256, diagonal: 20, out: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "diag_dim1_M256_N256_diagonal20_outTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.308
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        74.056
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.23
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.619
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.01
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        73.109
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.278
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        72.805
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.682
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        64.35
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.466
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.997
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.102
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        74.965
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.238
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        70.471
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.277
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        69.771
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.23
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        68.806
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.426
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.667
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.481
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.619
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.316
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        70.873
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.398
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        74.515
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.306
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        67.306
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.087
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        67.382
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.53
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.143
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.632
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.686
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.172
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        74.748
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.129
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.276
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.088
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        66.262
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.305
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        68.3
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.883
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        64.192
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.847
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.555
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.21
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.336
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        67.986
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        68.174
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.274
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        77.031
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        68.524
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        68.737
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        82.182
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        82.658
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        72.586
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        72.555
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.75
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        74.906
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        69.706
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        70.134
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.445
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.372
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        70.708
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        70.897
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.623
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.489
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.107
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.659
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.346
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.612
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        178.582
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        180.44
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.229
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.365
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        181.335
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        179.821
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        82.567
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        83.003
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        189.392
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        185.543
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.065
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        75.373
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        176.823
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        176.556
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.47
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.903
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        179.265
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        176.531
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.731
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.256
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        183.904
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        184.228
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embeddingbag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.532
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.547
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.017
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.456
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.496
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.099
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.534
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.553
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.051
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.534
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.607
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.071
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.317
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.383
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.358
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings10_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.021
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.362
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        62.029
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings120_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        159.329
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        162.716
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        168.331
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings1000_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        194.622
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        198.09
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        200.681
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "embedding_num_embeddings2300_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "N: 1, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fill__N1_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.923
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "N: 1, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fill__N1_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "N: 1024, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fill__N1024_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3.141
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "N: 1024, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fill__N1024_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "N: 2048, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fill__N2048_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3.286
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "N: 2048, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fill__N2048_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, dim: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "gather_M256_N512_dim0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        113.118
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, dim: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "gather_M256_N512_dim0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "gather_M512_N512_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        72.389
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "gather_M512_N512_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), num_groups: 2",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "GroupNormBenchmark_dims(32,8,16)_num_groups2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        61.443
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), num_groups: 2",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "GroupNormBenchmark_dims(32,8,16)_num_groups2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), num_groups: 4",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "GroupNormBenchmark_dims(32,8,16)_num_groups4",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.151
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), num_groups: 4",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "GroupNormBenchmark_dims(32,8,16)_num_groups4",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), num_groups: 2",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "GroupNormBenchmark_dims(32,8,56,56)_num_groups2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        114.533
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), num_groups: 2",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "GroupNormBenchmark_dims(32,8,56,56)_num_groups2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), num_groups: 4",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "GroupNormBenchmark_dims(32,8,56,56)_num_groups4",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        115.234
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), num_groups: 4",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "GroupNormBenchmark_dims(32,8,56,56)_num_groups4",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Hardsigmoid_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        67.826
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Hardsigmoid_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Hardsigmoid_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.515
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Hardsigmoid_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Hardswish_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        66.787
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Hardswish_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Hardswish_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        83.113
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Hardswish_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 8, N: 32, K: 1, dim: 0, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_add__M8_N32_K1_dim0_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.843
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 8, N: 32, K: 1, dim: 0, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_add__M8_N32_K1_dim0_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, dim: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_add__M256_N512_K1_dim1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.827
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, dim: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_add__M256_N512_K1_dim1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 1, dim: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_add__M512_N512_K1_dim2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        109.014
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 1, dim: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_add__M512_N512_K1_dim2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, K: 1, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M8_N8_K1_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.827
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, K: 1, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M8_N8_K1_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M256_N512_K1_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.455
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M256_N512_K1_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 1, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M512_N512_K1_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        113.076
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 1, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M512_N512_K1_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, K: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M8_N8_K2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.853
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, K: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M8_N8_K2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M256_N512_K2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        217.087
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M256_N512_K2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M512_N512_K2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        435.742
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, dim: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "index_select_M512_N512_K2_dim1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 16)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "InstanceNormBenchmark_dims(32,8,16)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        173.335
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 16)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "InstanceNormBenchmark_dims(32,8,16)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "InstanceNormBenchmark_dims(32,8,56,56)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        367.504
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "InstanceNormBenchmark_dims(32,8,56,56)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.007
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        12.8
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        46.536
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        23.278
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.093
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        35.263
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        129.568
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        193.034
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        489.112
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.555
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        112.852
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        288.879
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        138.231
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        209.454
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        545.522
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.475
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        119.065
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        320.41
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.255
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.838
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.54
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.015
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.965
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.491
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        82.641
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        115.622
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        293.062
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        80.7
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        112.755
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        294.516
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        82.42
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        119.435
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        323.407
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        83.009
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modenearest",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        119.971
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: linear",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modelinear",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        321.8
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: bicubic",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modebicubic",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.979
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        24.608
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,60,40)_output_size(24,24)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        143.411
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        101.105
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,600,400)_output_size(240,240)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        152.392
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        104.888
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 3, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,3,320,320)_output_size(256,256)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.146
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.244
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 60, 40), output_size: (24, 24), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,60,40)_output_size(24,24)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        99.362
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        99.985
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 600, 400), output_size: (240, 240), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,600,400)_output_size(240,240)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        103.578
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: True, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastTrue_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        102.833
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "uint8",
      "extra_info": {
        "input_config": "input_size: (1, 1, 320, 320), output_size: (256, 256), channels_last: False, mode: nearest, dtype: torch.uint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "interpolate_input_size(1,1,320,320)_output_size(256,256)_channels_lastFalse_modenearest_dtypetorch.uint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (1, 8, 16)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "LayerNormBenchmark_dims(1,8,16)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.502
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (1, 8, 16)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "LayerNormBenchmark_dims(1,8,16)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (8, 8, 16)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "LayerNormBenchmark_dims(8,8,16)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.515
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (8, 8, 16)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "LayerNormBenchmark_dims(8,8,16)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 16)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "LayerNormBenchmark_dims(32,8,16)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        60.72
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (32, 8, 16)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "LayerNormBenchmark_dims(32,8,16)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (64, 128, 56, 56)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "LayerNormBenchmark_dims(64,128,56,56)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        2676.907
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "dims: (64, 128, 56, 56)",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "LayerNormBenchmark_dims(64,128,56,56)",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, IN: 1, OUT: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "linear_N1_IN1_OUT1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.9
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, IN: 1, OUT: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "linear_N1_IN1_OUT1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, IN: 256, OUT: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "linear_N4_IN256_OUT128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        72.341
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, IN: 256, OUT: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "linear_N4_IN256_OUT128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 16, IN: 512, OUT: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "linear_N16_IN512_OUT256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        159.969
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 16, IN: 512, OUT: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "linear_N16_IN512_OUT256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, trans_a: True, trans_b: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "matmul_M1_N1_K1_trans_aTrue_trans_bFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.156
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, trans_a: True, trans_b: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "matmul_M1_N1_K1_trans_aTrue_trans_bFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 128, N: 128, K: 128, trans_a: True, trans_b: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "matmul_M128_N128_K128_trans_aTrue_trans_bFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        131.236
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 128, N: 128, K: 128, trans_a: True, trans_b: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "matmul_M128_N128_K128_trans_aTrue_trans_bFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 256, K: 256, trans_a: False, trans_b: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "matmul_M256_N256_K256_trans_aFalse_trans_bTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4498.939
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 256, K: 256, trans_a: False, trans_b: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "matmul_M256_N256_K256_trans_aFalse_trans_bTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mm_M1_N1_K1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.794
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mm_M1_N1_K1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mm_M64_N64_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.697
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mm_M64_N64_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mm_M64_N64_K128_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        60.233
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "mm_M64_N64_K128_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.219
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.523
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.981
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.316
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.116
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.478
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.781
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.355
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M16_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.22
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.816
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.687
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.519
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.397
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.681
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.839
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.332
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num_M64_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.162
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.5
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.155
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.427
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.177
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.59
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.77
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.909
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 16, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M16_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.066
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.877
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.939
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        12.505
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.034
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float32_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.008
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float32, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float32_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.481
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float64_replace_infTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        12.689
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, dtype: torch.float64, replace_inf: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "nan_to_num__M64_N64_dtypetorch.float64_replace_infFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: 3, stride: 1, N: 8, C: 256, L: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MaxPool1d_kernel3_stride1_N8_C256_L256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        123.765
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: 3, stride: 1, N: 8, C: 256, L: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MaxPool1d_kernel3_stride1_N8_C256_L256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: 3, stride: 1, N: 8, C: 256, L: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AvgPool1d_kernel3_stride1_N8_C256_L256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        316.869
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: 3, stride: 1, N: 8, C: 256, L: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AvgPool1d_kernel3_stride1_N8_C256_L256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.182
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AvgPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.588
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AvgPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AdaptiveMaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        64.549
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AdaptiveMaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FractionalMaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        67.243
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1], stride: [2, 1], N: 1, C: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FractionalMaxPool2d_kernel[3,1]_stride[2,1]_N1_C16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        242.332
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AvgPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        102.31
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AvgPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AdaptiveMaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        192.949
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "AdaptiveMaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FractionalMaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        66.774
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "kernel: [3, 1, 3], stride: [2, 1, 2], N: 1, C: 16, D: 16, H: 32, W: 32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FractionalMaxPool3d_kernel[3,1,3]_stride[2,1,2]_N1_C16_D16_H32_W32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M1_N1_K1_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.886
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M1_N1_K1_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M1_N1_K1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.906
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M1_N1_K1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M1_N1_K1_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.895
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M1_N1_K1_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K64_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        131.671
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K64_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        152.25
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K64_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        746.778
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K64_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K128_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        210.65
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K128_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K128_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        253.465
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K128_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K128_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1495.073
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "fmod_M64_N64_K128_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M1_N1_K1_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.888
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M1_N1_K1_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M1_N1_K1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.895
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M1_N1_K1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M1_N1_K1_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1.913
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 1, N: 1, K: 1, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M1_N1_K1_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K64_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        147.992
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K64_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        171.655
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K64_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        867.007
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 64, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K64_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K128_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        247.891
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "int32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.int32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K128_cpu_dtypetorch.int32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K128_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        293.562
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K128_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K128_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1738.812
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float64",
      "extra_info": {
        "input_config": "M: 64, N: 64, K: 128, device: cpu, dtype: torch.float64",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "remainder_M64_N64_K128_cpu_dtypetorch.float64",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Softmax_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        125.051
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Softmax_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Softmax_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        323.243
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Softmax_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Softmax2d_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        122.993
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Softmax2d_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Softmax2d_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        320.646
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "Softmax2d_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "LogSoftmax_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        163.908
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "LogSoftmax_N1_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "LogSoftmax_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        268.845
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, C: 3, H: 256, W: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "LogSoftmax_N4_C3_H256_W256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, parts: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "split_M8_N8_parts2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.861
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 8, parts: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "split_M8_N8_parts2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, parts: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "split_M256_N512_parts2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.94
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 256, N: 512, parts: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "split_M256_N512_parts2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, parts: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "split_M512_N512_parts2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.02
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, parts: 2, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "split_M512_N512_parts2_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, device: cpu, dim: 0",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(1,1,1)_N2_cpu_dim0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.812
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, device: cpu, dim: 0",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(1,1,1)_N2_cpu_dim0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, device: cpu, dim: 1",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(1,1,1)_N2_cpu_dim1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.27
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, device: cpu, dim: 1",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(1,1,1)_N2_cpu_dim1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, device: cpu, dim: 2",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(1,1,1)_N2_cpu_dim2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.379
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, device: cpu, dim: 2",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(1,1,1)_N2_cpu_dim2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, device: cpu, dim: 3",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(1,1,1)_N2_cpu_dim3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.89
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (1, 1, 1), N: 2, device: cpu, dim: 3",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(1,1,1)_N2_cpu_dim3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, device: cpu, dim: 0",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(512,512,2)_N2_cpu_dim0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        102.15
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, device: cpu, dim: 0",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(512,512,2)_N2_cpu_dim0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, device: cpu, dim: 1",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(512,512,2)_N2_cpu_dim1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        101.988
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, device: cpu, dim: 1",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(512,512,2)_N2_cpu_dim1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, device: cpu, dim: 2",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(512,512,2)_N2_cpu_dim2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        227.726
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, device: cpu, dim: 2",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(512,512,2)_N2_cpu_dim2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, device: cpu, dim: 3",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(512,512,2)_N2_cpu_dim3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        173.95
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (512, 512, 2), N: 2, device: cpu, dim: 3",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(512,512,2)_N2_cpu_dim3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, device: cpu, dim: 0",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(128,1024,2)_N2_cpu_dim0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        102.68
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, device: cpu, dim: 0",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(128,1024,2)_N2_cpu_dim0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, device: cpu, dim: 1",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(128,1024,2)_N2_cpu_dim1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        101.402
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, device: cpu, dim: 1",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(128,1024,2)_N2_cpu_dim1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, device: cpu, dim: 2",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(128,1024,2)_N2_cpu_dim2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        155.919
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, device: cpu, dim: 2",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(128,1024,2)_N2_cpu_dim2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, device: cpu, dim: 3",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(128,1024,2)_N2_cpu_dim3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        126.527
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "sizes: (128, 1024, 2), N: 2, device: cpu, dim: 3",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "stack_sizes(128,1024,2)_N2_cpu_dim3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 32, dim: 0, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V32_dim0_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.835
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 32, dim: 0, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V32_dim0_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 32, dim: 0, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V32_dim0_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.612
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 32, dim: 0, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V32_dim0_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 32, dim: 1, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V32_dim1_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.858
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 32, dim: 1, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V32_dim1_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 32, dim: 1, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V32_dim1_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.914
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 32, dim: 1, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V32_dim1_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 512, dim: 0, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V512_dim0_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        46.834
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 512, dim: 0, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V512_dim0_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 512, dim: 0, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V512_dim0_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        51.587
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 512, dim: 0, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V512_dim0_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 512, dim: 1, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V512_dim1_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        46.612
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 512, dim: 1, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V512_dim1_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 512, dim: 1, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V512_dim1_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.402
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 64, V: 512, dim: 1, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R64_V512_dim1_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 32, dim: 0, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V32_dim0_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.577
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 32, dim: 0, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V32_dim0_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 32, dim: 0, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V32_dim0_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.148
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 32, dim: 0, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V32_dim0_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 32, dim: 1, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V32_dim1_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.762
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 32, dim: 1, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V32_dim1_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 32, dim: 1, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V32_dim1_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.742
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 32, dim: 1, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V32_dim1_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 512, dim: 0, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V512_dim0_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        50.886
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 512, dim: 0, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V512_dim0_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 512, dim: 0, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V512_dim0_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.845
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 512, dim: 0, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V512_dim0_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 512, dim: 1, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V512_dim1_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.768
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 512, dim: 1, contiguous: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V512_dim1_contiguousTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 512, dim: 1, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V512_dim1_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.672
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "R: 256, V: 512, dim: 1, contiguous: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sum_R256_V512_dim1_contiguousFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M8_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.97
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M8_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M8_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.054
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M8_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M8_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.196
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M8_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M16_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.012
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M16_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M16_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.293
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M16_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M16_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.583
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M16_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M32_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.117
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M32_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M32_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.591
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M32_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M32_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.271
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FloatToHalfTensorConversionBenchmark_M32_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M8_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.084
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M8_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M8_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.325
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M8_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M8_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.673
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 8, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M8_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M16_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.133
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M16_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M16_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.699
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M16_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M16_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.504
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 16, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M16_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M32_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.394
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M32_N16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M32_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.481
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M32_N64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M32_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.571
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 32, N: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HalfToFloatTensorConversionBenchmark_M32_N128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcmul_M1_N2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.687
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcmul_M1_N2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "M: 1, N: 2, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcmul_M1_N2_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.611
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "M: 1, N: 2, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcmul_M1_N2_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcmul_M32_N64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.389
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcmul_M32_N64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcmul_M32_N64_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.924
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcmul_M32_N64_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcdiv_M1_N2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.422
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 1, N: 2, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcdiv_M1_N2_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "M: 1, N: 2, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcdiv_M1_N2_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.569
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "M: 1, N: 2, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcdiv_M1_N2_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcdiv_M32_N64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.29
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcdiv_M32_N64_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcdiv_M32_N64_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.777
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "bfloat16",
      "extra_info": {
        "input_config": "M: 32, N: 64, device: cpu, dtype: torch.bfloat16",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "addcdiv_M32_N64_cpu_dtypetorch.bfloat16",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "shape: (16, 4), k: 4, dim: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "topk_shape(16,4)_k4_dim1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.064
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "shape: (16, 4), k: 4, dim: 1, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "topk_shape(16,4)_k4_dim1_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "shape: (1048576,), k: 16, dim: 0, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "topk_shape(1048576,)_k16_dim0_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3774.178
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "shape: (1048576,), k: 16, dim: 0, device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "topk_shape(1048576,)_k16_dim0_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "cond_shape: (8, 16, 1), input_shape: (1,), other_shape: (1,), device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "where_cond_shape(8,16,1)_input_shape(1,)_other_shape(1,)_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.472
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "cond_shape: (8, 16, 1), input_shape: (1,), other_shape: (1,), device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "where_cond_shape(8,16,1)_input_shape(1,)_other_shape(1,)_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "cond_shape: (8, 16, 1), input_shape: (16, 1), other_shape: (8, 16, 1), device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "where_cond_shape(8,16,1)_input_shape(16,1)_other_shape(8,16,1)_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.619
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "cond_shape: (8, 16, 1), input_shape: (16, 1), other_shape: (8, 16, 1), device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "where_cond_shape(8,16,1)_input_shape(16,1)_other_shape(8,16,1)_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "cond_shape: (8, 16, 1), input_shape: (8, 1, 1), other_shape: (1,), device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "where_cond_shape(8,16,1)_input_shape(8,1,1)_other_shape(1,)_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.656
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float32",
      "extra_info": {
        "input_config": "cond_shape: (8, 16, 1), input_shape: (8, 1, 1), other_shape: (1,), device: cpu, dtype: torch.float32",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "where_cond_shape(8,16,1)_input_shape(8,1,1)_other_shape(1,)_cpu_dtypetorch.float32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.858
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.912
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.884
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.3
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.221
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.22
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.417
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        93.052
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        84.585
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        80.463
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        97.822
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        105.441
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.561
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.564
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.523
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.041
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.085
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.93
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.814
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        85.892
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        86.223
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        87.21
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        86.23
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        108.197
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu6_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.706
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.694
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.699
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.551
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.505
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.31
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        79.76
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        84.643
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        85.118
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        85.644
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        85.604
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        106.002
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardtanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.174
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4.996
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.349
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.686
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.621
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.653
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        345.409
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        361.199
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        360.659
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        345.578
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        360.612
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        360.98
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardsigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.844
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.794
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.564
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        12.195
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.755
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.511
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        440.92
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        445.671
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        524.209
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        446.411
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        449.516
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        544.538
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.leaky_relu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.133
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        5.996
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.074
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.789
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.16
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.988
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        324.621
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        417.707
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        413.89
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        324.894
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        413.154
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        417.372
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.sigmoid_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.181
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.344
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.509
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.791
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.208
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.13
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        410.681
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        371.845
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        372.103
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        411.555
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        376.982
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        375.163
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.tanh_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.702
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.642
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.606
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.349
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.48
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.586
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        336.085
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        351.605
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        351.744
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        339.172
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        351.442
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        355.115
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.hardswish_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.31
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.627
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.74
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.983
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.787
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.843
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        158.937
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        453.694
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        451.494
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        157.509
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        455.899
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        448.107
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.elu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.816
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.283
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.4
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.766
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.528
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.616
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (2, 3, 4, 5), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(2,3,4,5)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        156.521
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        449.723
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        449.263
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (512, 512), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(512,512)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        156.122
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        445.899
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        457.025
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "dims: (256, 1024), contig: False, inplace: False, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "functional.celu_dims(256,1024)_contigFalse_inplaceFalse_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.253
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        50.363
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.7
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        50.171
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.683
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.228
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.322
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.855
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.44
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.718
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.005
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.595
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        62.305
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.761
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        62.204
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.375
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.659
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        89.546
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        217.095
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        61.504
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        215.818
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        61.633
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        188.642
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        691.329
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.415
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.173
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.303
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.26
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.455
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.121
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.063
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.367
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.882
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.214
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.947
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.491
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.283
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.707
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.511
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.866
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        42.53
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        89.642
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        185.954
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        621.588
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        398.899
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        626.844
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        397.949
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        695.282
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_relu_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.761
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.398
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        28.157
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.151
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.701
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.345
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.044
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.191
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        419.308
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        395.605
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.966
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.267
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.01
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.124
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        428.355
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        379.743
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.86
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.123
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        329.282
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        73.213
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        703.917
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        417.097
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        141.933
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        74.988
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.762
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.167
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.349
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.682
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.1
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.407
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.448
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.023
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.399
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.5
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.379
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.379
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        19.263
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.58
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.866
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.128
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.855
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.054
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        122.912
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        76.652
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        83.247
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.61
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        86.936
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        60.871
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "add_scalar_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.165
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.349
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.177
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.559
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.201
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.531
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 2, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N2_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.176
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.232
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.313
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.262
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.17
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.321
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N8_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.296
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.908
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.785
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.08
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.423
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.94
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N64_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        82.011
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.238
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        81.674
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.qint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.561
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.qint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        86.379
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.qint32_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        61.089
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 512, dtype: torch.qint32, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "mul_scalar_N512_dtypetorch.qint32_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        280.066
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        327.155
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        281.335
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        323.992
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        281.797
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        322.207
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        316.035
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        359.934
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        316.378
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        358.837
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        316.037
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        359.603
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        518.835
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        557.449
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        519.391
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        557.31
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        523.092
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        575.499
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        539.116
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        576.764
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        538.332
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        578.525
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        541.202
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        596.115
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        78.043
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        78.404
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        79.996
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        79.964
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        83.308
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        83.445
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        88.895
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        88.871
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        89.064
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        89.406
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        94.08
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        93.629
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        322.484
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        320.455
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        322.015
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        323.383
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        328.928
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        328.5
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        319.753
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        315.398
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        318.627
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        319.197
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        324.21
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        325.721
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        264.269
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        266.672
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        268.161
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        300.049
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        300.138
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        302.019
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        502.619
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        504.885
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        505.251
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        525.939
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        526.701
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        529.116
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        64.109
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        65.395
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        71.26
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 10, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings10_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        73.539
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        74.981
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        80.727
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 120, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings120_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        297.911
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        300.947
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        310.193
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 1000, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings1000_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        334.272
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        338.29
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 16, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size16_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        342.746
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 2300, embedding_dim: 64, input_size: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qatEmbedding_num_embeddings2300_embedding_dim64_input_size64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, dtype: torch.qint8",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QBatchNorm1d_M1_N256_K3136_cpu_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1282.382
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, dtype: torch.qint8",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QBatchNorm1d_M1_N256_K3136_cpu_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, dtype: torch.qint8",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QBatchNorm2d_M1_N256_K3136_cpu_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1147.115
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 1, N: 256, K: 3136, device: cpu, dtype: torch.qint8",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QBatchNorm2d_M1_N256_K3136_cpu_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        240.002
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        240.621
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        943.038
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: all, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigall_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        309.568
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        309.498
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1022.238
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: one, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contigone_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        372.112
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        373.382
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1103.606
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 256, N: 512, K: 1, L: 2, dim: 0, contig: none, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M256_N512_K1_L2_dim0_contignone_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        375.408
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        375.6
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1669.071
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: all, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigall_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        483.811
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        483.215
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1838.274
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: one, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contigone_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        577.465
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        582.25
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1980.909
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "M: 512, N: 512, K: 2, L: 1, dim: 1, contig: none, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qcat_M512_N512_K2_L1_dim1_contignone_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.094
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.673
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.381
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.333
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.755
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.149
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.033
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.347
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.013
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.857
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.395
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.398
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.816
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.115
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.051
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.423
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.55
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.393
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.606
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.382
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.084
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.817
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.439
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.703
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.943
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        30.456
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.622
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        33.644
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.927
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        23.394
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.708
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.494
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.874
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.165
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.63
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        34.843
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.044
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        23.518
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.853
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.679
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.226
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.325
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.245
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        38.941
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.723
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        33.285
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.025
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.164
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "eq_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.989
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.608
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.301
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.819
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.623
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.697
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.959
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.657
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.799
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.779
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.524
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.097
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.526
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.687
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.848
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.624
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.623
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.324
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.453
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.17
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.92
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.441
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.184
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.107
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.585
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        29.924
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.021
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        34.351
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.414
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.837
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        15.989
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.617
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.252
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        31.74
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.906
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        35.631
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.163
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.65
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.015
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.633
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.742
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.384
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.186
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.323
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.663
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.937
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.274
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.637
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ne_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.043
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.636
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.45
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.24
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.841
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.6
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.802
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.544
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.037
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.59
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.517
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.527
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.455
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.569
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.827
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.696
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.599
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.323
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.626
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.557
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.772
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.362
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.466
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.09
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.865
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        30.09
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.632
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        36.21
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.032
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.568
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.675
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.697
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.458
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        31.864
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.346
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        36.94
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.281
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.823
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.502
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        27.56
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.973
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.073
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.822
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.743
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.42
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.768
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.514
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        33.504
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "lt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.889
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.345
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.471
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.785
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.417
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.575
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.985
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.132
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.868
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.548
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.329
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.946
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.697
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.77
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.794
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.062
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.539
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.136
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.579
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.884
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.744
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.419
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.241
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.491
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.734
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        30.023
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.571
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        33.091
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.483
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.753
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.119
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.041
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.301
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        31.638
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.709
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        34.138
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.124
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.707
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.064
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.28
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.973
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.365
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.529
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        38.486
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.317
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.347
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.417
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        31.43
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "gt_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.108
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.654
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.583
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.925
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.658
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.841
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.868
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.211
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.205
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.632
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.531
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.945
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.718
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.943
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.752
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.147
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.723
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.23
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        14.469
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.955
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.295
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.636
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.075
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.625
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.249
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        30.373
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.266
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        33.19
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.821
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.904
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.485
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.38
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.808
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        31.989
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.958
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        34.335
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.828
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        23.187
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.522
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.456
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.264
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.74
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.47
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        38.586
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.565
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.985
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.839
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.124
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "le_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.868
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.465
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.963
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.004
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.451
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.741
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.744
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.153
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.826
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.598
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.969
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.255
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.453
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.729
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.815
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.361
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.421
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.006
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.963
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.247
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6.954
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.443
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        11.044
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.607
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 8, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N8_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.829
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        30.216
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.418
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        33.727
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.176
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.72
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        15.545
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.393
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.quint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.quint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        17.323
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        31.715
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.147
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        34.711
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10.071
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        22.754
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.14
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        26.66
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint8, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint8_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.831
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.298
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigFalse_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        25.982
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.033
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: False, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigFalse_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21.775
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        32.907
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: False, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigTrue_other_scalarFalse_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        20.171
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        31.559
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 64, dtype: torch.qint32, contig: True, other_scalar: True, out_variant: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "ge_N64_dtypetorch.qint32_contigTrue_other_scalarTrue_out_variantTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 128, OC: 256, kernel: 3, stride: 1, N: 1, L: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QConv1d_IC128_OC256_kernel3_stride1_N1_L64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        2548.02
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 128, OC: 256, kernel: 3, stride: 1, N: 1, L: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QConv1d_IC128_OC256_kernel3_stride1_N1_L64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 2, N: 4, L: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QConv1d_IC256_OC256_kernel3_stride2_N4_L64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        10522.958
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 2, N: 4, L: 64, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QConv1d_IC256_OC256_kernel3_stride2_N4_L64_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QConv2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        3102.297
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "IC: 256, OC: 256, kernel: 3, stride: 1, N: 1, H: 16, W: 16, G: 1, pad: 0, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QConv2d_IC256_OC256_kernel3_stride1_N1_H16_W16_G1_pad0_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        18.103
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        35.096
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        66.285
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        36.885
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        68.281
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        129.462
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        37.732
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        68.488
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        130.177
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.548
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.846
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        13.758
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.775
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        110.286
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        213.059
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        111.627
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim128",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        214.625
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim256",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        420.122
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim512",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        207.741
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        365.048
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        673.619
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_prepack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.873
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.469
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.013
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_prepack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.843
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.959
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.017
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_prepack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        23.226
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.529
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        71.996
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_byte_unpack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.004
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.123
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        8.979
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_4bit_unpack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        12.877
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 128, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim128_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        12.877
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 256, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim256_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        12.767
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "num_embeddings: 80, embedding_dim: 512, batch_size: 10",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qembeddingbag_2bit_unpack_num_embeddings80_embedding_dim512_batch_size10",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.49
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.59
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.403
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.599
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.537
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.691
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.425
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.582
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.828
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.17
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.927
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.097
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 10, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags10_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.338
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.575
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.373
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.482
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.398
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.596
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.293
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.679
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.945
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.208
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.951
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.295
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 120, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags120_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.247
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.61
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.201
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.508
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.345
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.421
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.542
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.53
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.118
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.318
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.061
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.34
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 1000, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags1000_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.547
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.548
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.373
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.663
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 8, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size8_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.528
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.547
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.454
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.667
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 16, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size16_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        39.979
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.386
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: True, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseTrue_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.59
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: True, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetTrue_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        40.336
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "embeddingbags: 2300, dim: 64, mode: sum, input_size: 64, offset: 0, sparse: False, include_last_offset: False, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "qEmbeddingBag_embeddingbags2300_dim64_modesum_input_size64_offset0_sparseFalse_include_last_offsetFalse_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), num_groups: 2, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QGroupNormBenchmark_dims(32,8,16)_num_groups2_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.689
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), num_groups: 2, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QGroupNormBenchmark_dims(32,8,16)_num_groups2_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), num_groups: 4, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QGroupNormBenchmark_dims(32,8,16)_num_groups4_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.477
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), num_groups: 4, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QGroupNormBenchmark_dims(32,8,16)_num_groups4_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), num_groups: 2, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QGroupNormBenchmark_dims(32,8,56,56)_num_groups2_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1148.775
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), num_groups: 2, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QGroupNormBenchmark_dims(32,8,56,56)_num_groups2_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), num_groups: 4, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QGroupNormBenchmark_dims(32,8,56,56)_num_groups4_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1150.244
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), num_groups: 4, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QGroupNormBenchmark_dims(32,8,56,56)_num_groups4_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QInstanceNormBenchmark_dims(32,8,16)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.357
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QInstanceNormBenchmark_dims(32,8,16)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QInstanceNormBenchmark_dims(32,8,56,56)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1148.037
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 56, 56), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QInstanceNormBenchmark_dims(32,8,56,56)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, K: 32, dtype: torch.quint8, mode: nearest, scale: 0.5, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M32_N32_K32_dtypetorch.quint8_modenearest_scale0.5_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.292
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, K: 32, dtype: torch.quint8, mode: nearest, scale: 0.5, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M32_N32_K32_dtypetorch.quint8_modenearest_scale0.5_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, K: 32, dtype: torch.quint8, mode: bilinear, scale: 0.5, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M32_N32_K32_dtypetorch.quint8_modebilinear_scale0.5_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.43
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, K: 32, dtype: torch.quint8, mode: bilinear, scale: 0.5, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M32_N32_K32_dtypetorch.quint8_modebilinear_scale0.5_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, K: 32, dtype: torch.quint8, mode: nearest, scale: 2.0, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M32_N32_K32_dtypetorch.quint8_modenearest_scale2.0_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        7.335
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, K: 32, dtype: torch.quint8, mode: nearest, scale: 2.0, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M32_N32_K32_dtypetorch.quint8_modenearest_scale2.0_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, K: 32, dtype: torch.quint8, mode: bilinear, scale: 2.0, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M32_N32_K32_dtypetorch.quint8_modebilinear_scale2.0_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        9.656
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, K: 32, dtype: torch.quint8, mode: bilinear, scale: 2.0, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M32_N32_K32_dtypetorch.quint8_modebilinear_scale2.0_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 3, N: 720, K: 1280, dtype: torch.quint8, mode: bilinear, scale: 0.83333, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M3_N720_K1280_dtypetorch.quint8_modebilinear_scale0.83333_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        66.738
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 3, N: 720, K: 1280, dtype: torch.quint8, mode: bilinear, scale: 0.83333, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_interpolate_M3_N720_K1280_dtypetorch.quint8_modebilinear_scale0.83333_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (1, 8, 16), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLayerNormBenchmark_dims(1,8,16)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        16.631
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (1, 8, 16), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLayerNormBenchmark_dims(1,8,16)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (8, 8, 16), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLayerNormBenchmark_dims(8,8,16)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        64.892
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (8, 8, 16), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLayerNormBenchmark_dims(8,8,16)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLayerNormBenchmark_dims(32,8,16)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        66.794
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (32, 8, 16), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLayerNormBenchmark_dims(32,8,16)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (64, 128, 56, 56), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLayerNormBenchmark_dims(64,128,56,56)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        51336.492
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "dims: (64, 128, 56, 56), dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLayerNormBenchmark_dims(64,128,56,56)_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, IN: 1, OUT: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLinear_N1_IN1_OUT1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        47.072
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, IN: 1, OUT: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLinear_N1_IN1_OUT1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, IN: 256, OUT: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLinear_N4_IN256_OUT128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        93.137
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, IN: 256, OUT: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLinear_N4_IN256_OUT128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 16, IN: 512, OUT: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLinear_N16_IN512_OUT256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        94.653
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 16, IN: 512, OUT: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLinear_N16_IN512_OUT256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, IN: 1, OUT: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QDynamicLinear_N1_IN1_OUT1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.648
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, IN: 1, OUT: 1, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QDynamicLinear_N1_IN1_OUT1_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, IN: 256, OUT: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QDynamicLinear_N4_IN256_OUT128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        187.471
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 4, IN: 256, OUT: 128, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QDynamicLinear_N4_IN256_OUT128_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 16, IN: 512, OUT: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QDynamicLinear_N16_IN512_OUT256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        190.56
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 16, IN: 512, OUT: 256, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "QDynamicLinear_N16_IN512_OUT256_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        177.723
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        167.524
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MovingAverageMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        208.246
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MovingAverageMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MovingAverageMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        198.034
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MovingAverageMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "PerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        403.126
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "PerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "PerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        383.861
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "PerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MovingAveragePerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        416.43
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MovingAveragePerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MovingAveragePerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        421.88
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_channel_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "MovingAveragePerChannelMinMaxObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_channel_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HistogramObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1837.221
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HistogramObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HistogramObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1884.559
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HistogramObserver_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1721.079
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_affine",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_affine",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1800.916
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, device: cpu, qscheme: torch.per_tensor_symmetric",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "HistogramObserverCalculateQparams_C3_M512_N512_dtypetorch.quint8_cpu_qschemetorch.per_tensor_symmetric",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 4, C: 3, input_size: (224, 224), output_size: (112, 112), contig: True, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAdaptiveAvgPool2dBenchmark_N4_C3_input_size(224,224)_output_size(112,112)_contigTrue_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        125.536
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "N: 4, C: 3, input_size: (224, 224), output_size: (112, 112), contig: True, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAdaptiveAvgPool2dBenchmark_N4_C3_input_size(224,224)_output_size(112,112)_contigTrue_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 4, C: 3, input_size: (224, 224), output_size: (112, 112), contig: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAdaptiveAvgPool2dBenchmark_N4_C3_input_size(224,224)_output_size(112,112)_contigTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        121.315
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "N: 4, C: 3, input_size: (224, 224), output_size: (112, 112), contig: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAdaptiveAvgPool2dBenchmark_N4_C3_input_size(224,224)_output_size(112,112)_contigTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 4, C: 3, input_size: (224, 224), output_size: (112, 112), contig: True, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAdaptiveAvgPool2dBenchmark_N4_C3_input_size(224,224)_output_size(112,112)_contigTrue_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        121.851
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "N: 4, C: 3, input_size: (224, 224), output_size: (112, 112), contig: True, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAdaptiveAvgPool2dBenchmark_N4_C3_input_size(224,224)_output_size(112,112)_contigTrue_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAvgPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.881
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAvgPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAvgPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.53
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAvgPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAvgPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.155
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QAvgPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMaxPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.36
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint32",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.qint32",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMaxPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.qint32",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMaxPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        62.653
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMaxPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMaxPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.088
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 1, H: 3, W: 3, k: (3, 3), s: (1, 1), p: (0, 0), N: 2, contig: True, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMaxPool2dBenchmark_C1_H3_W3_k(3,3)_s(1,1)_p(0,0)_N2_contigTrue_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "I: 1, H: 3, NL: 1, B: True, D: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLSTM_I1_H3_NL1_BTrue_DFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        21549.898
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "I: 1, H: 3, NL: 1, B: True, D: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLSTM_I1_H3_NL1_BTrue_DFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "I: 1, H: 3, NL: 1, B: True, D: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLSTM_I1_H3_NL1_BTrue_DTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        43358.336
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "I: 1, H: 3, NL: 1, B: True, D: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLSTM_I1_H3_NL1_BTrue_DTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "I: 5, H: 7, NL: 4, B: True, D: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLSTM_I5_H7_NL4_BTrue_DFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        85004.382
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "I: 5, H: 7, NL: 4, B: True, D: False, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLSTM_I5_H7_NL4_BTrue_DFalse_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "I: 5, H: 7, NL: 4, B: True, D: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLSTM_I5_H7_NL4_BTrue_DTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        170651.775
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "qint8",
      "extra_info": {
        "input_config": "I: 5, H: 7, NL: 4, B: True, D: True, dtype: torch.qint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QLSTM_I5_H7_NL4_BTrue_DTrue_dtypetorch.qint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMethodTensorInputCopyBenchmark_M32_N32_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        0.892
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, dtype: torch.quint8, contig: False",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMethodTensorInputCopyBenchmark_M32_N32_dtypetorch.quint8_contigFalse",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMethodTensorInputCopyBenchmark_M32_N32_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        0.89
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 32, N: 32, dtype: torch.quint8, contig: True",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QMethodTensorInputCopyBenchmark_M32_N32_dtypetorch.quint8_contigTrue",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, mode: Q",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QuantizePerTensor_C3_M512_N512_dtypetorch.quint8_modeQ",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        150.404
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, mode: Q",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QuantizePerTensor_C3_M512_N512_dtypetorch.quint8_modeQ",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, mode: D",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "DequantizePerTensor_C3_M512_N512_dtypetorch.quint8_modeD",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        115.781
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, mode: D",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "DequantizePerTensor_C3_M512_N512_dtypetorch.quint8_modeD",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, mode: Q, axis: 0",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QuantizePerChannel_C3_M512_N512_dtypetorch.quint8_modeQ_axis0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        149.768
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, mode: Q, axis: 0",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "QuantizePerChannel_C3_M512_N512_dtypetorch.quint8_modeQ_axis0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, mode: D, axis: 0",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "DequantizePerChannel_C3_M512_N512_dtypetorch.quint8_modeD_axis0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        296.866
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "C: 3, M: 512, N: 512, dtype: torch.quint8, mode: D, axis: 0",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "DequantizePerChannel_C3_M512_N512_dtypetorch.quint8_modeD_axis0",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FakeQuantize_N1_C3_H512_W512_zero_point_dtypetorch.int32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        506.912
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "FakeQuantize_N1_C3_H512_W512_zero_point_dtypetorch.int32_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        214.961
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        215.026
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        211.998
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        212.361
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        677.924
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        680.202
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        679.974
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        681.155
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        678.685
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        679.309
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        679.037
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        683.86
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        411.738
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        412.568
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        412.796
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        415.389
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        411.735
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        412.446
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        416.36
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        410.227
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_tensor_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        466.54
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        467.665
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        455.476
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        455.886
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        769.497
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        772.456
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        780.228
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        787.909
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        784.708
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        780.473
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        770.906
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd2",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        770.863
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "learnable_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd3",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        408.349
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        412.669
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 4, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits4_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        415.369
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwdall",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        415.004
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "training",
      "dtype": "float",
      "extra_info": {
        "input_config": "N: 1, C: 3, H: 512, W: 512, zero_point_dtype: torch.int32, nbits: 8, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "original_kernel_channel_N1_C3_H512_W512_zero_point_dtypetorch.int32_nbits8_cpu_bwd1",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_argsort_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        500.874
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_argsort_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_clone_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.498
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_clone_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_mean_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        98.108
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_mean_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_relu_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        50.599
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_relu_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_relu__M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        51.865
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_relu__M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_sort_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        497.925
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "q_sort_M512_N512_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, k: 5, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qtopk_M512_N512_k5_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        108.107
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "quint8",
      "extra_info": {
        "input_config": "M: 512, N: 512, k: 5, dtype: torch.quint8",
        "device": "unknown",
        "arch": "unknown",
        "use_compile": false
      }
    },
    "model": {
      "name": "qtopk_M512_N512_k5_dtypetorch.quint8",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "abs_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.949
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "abs_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "abs__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.023
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "abs__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "acos_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        163.501
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "acos_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "acos__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        155.441
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "acos__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "argsort_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1297.719
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "argsort_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "asin_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        145.178
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "asin_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "asin__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        139.726
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "asin__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "atan_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        183.852
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "atan_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "atan__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        179.955
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "atan__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "ceil_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.274
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "ceil_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "ceil__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.188
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "ceil__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "clamp_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.085
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "clamp_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "clone_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        56.759
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "clone_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cos_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        154.941
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cos_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cos__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        151.332
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cos__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cosh_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        235.712
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cosh_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "digamma_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        513.434
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "digamma_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erf_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        251.022
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erf_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erf__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        247.054
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erf__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erfc_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        471.5
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erfc_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erfc__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        467.316
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erfc__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erfinv_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1355.529
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "erfinv_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "exp_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        103.993
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "exp_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "exp__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        100.54
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "exp__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "expm1_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        226.643
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "expm1_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "expm1__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        222.137
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "expm1__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "floor_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        53.725
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "floor_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "floor__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.35
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "floor__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "frac_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.747
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "frac_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "frac__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.21
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "frac__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "gelu_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        158.169
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "gelu_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "hardshrink_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.235
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "hardshrink_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "lgamma_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        847.449
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "lgamma_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        155.671
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log10_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        163.45
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log10_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log10__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        161.162
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log10__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log1p_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        165.047
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log1p_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log1p__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        161.334
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log1p__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log2_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        165.102
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log2_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log2__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        161.455
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log2__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        150.874
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "log__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logit_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        178.638
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logit_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logit__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        172.768
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "logit__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "neg_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.259
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "neg_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "neg__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        51.913
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "neg__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "reciprocal_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        63.946
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "reciprocal_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "reciprocal__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        59.63
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "reciprocal__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.744
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        53.116
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "relu__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "round_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.98
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "round_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "round__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        52.437
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "round__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "rsqrt_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        72.546
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "rsqrt_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "rsqrt__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        67.748
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "rsqrt__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sigmoid_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        104.607
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sigmoid_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sigmoid__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        104.214
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sigmoid__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sign_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        57.899
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sign_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sgn_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.028
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sgn_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sin_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        130.555
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sin_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sin__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        127.148
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sin__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sinh_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        236.47
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sinh_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sqrt_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.213
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sqrt_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sqrt__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.345
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sqrt__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "square_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.385
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "square_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "square__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        55.104
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "square__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "tan_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        212.032
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "tan_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "tan__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        207.94
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "tan__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "tanh_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        254.151
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "tanh_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "tanh__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        251.769
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "tanh__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "trunc_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        51.59
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "trunc_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "trunc__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        50.446
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "trunc__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "unique_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        18985.282
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "unique_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "zero__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        49.575
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "zero__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bernoulli__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        2762.499
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "bernoulli__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cauchy__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        6133.74
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "cauchy__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "digamma__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        969.262
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "digamma__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "exponential__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        4552.622
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "exponential__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "normal__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        1969.078
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "normal__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "random__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        740.752
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "random__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sign__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        54.186
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "sign__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "uniform__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        717.844
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "uniform__M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "half_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        58.75
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "half_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "long_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "latency",
      "unit": "us",
      "benchmark_values": [
        70.655
      ],
      "target_value": null
    }
  },
  {
    "benchmark": {
      "name": "PyTorch operator benchmark",
      "mode": "inference",
      "dtype": "float",
      "extra_info": {
        "input_config": "M: 512, N: 512, device: cpu",
        "device": "cpu",
        "arch": "aarch64",
        "use_compile": false
      }
    },
    "model": {
      "name": "long_M512_N512_cpu",
      "type": "micro-benchmark",
      "origins": [
        "pytorch"
      ]
    },
    "metric": {
      "name": "peak memory",
      "unit": "KB",
      "benchmark_values": [
        0.0
      ],
      "target_value": null
    }
  }
]